name: Evaluate
description: The function evaluates the model performance.
inputs:
- {name: test_results, type: results}
outputs:
- {name: mlpipeline_metrics, type: Metrics}
implementation:
  container:
    image: python:3.8
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'pandas' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
      --quiet --no-warn-script-location 'pandas' 'scikit-learn' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def evaluate(test_results,
                   mlpipeline_metrics):
          """The function evaluates the model performance."""

          import json
          import pandas as pd
          from sklearn.metrics import accuracy_score

          def classification_metrics_helper(df):

              accuracy = accuracy_score(df["y_test"], df["y_pred"])

              metrics={
                'metrics': [{
                'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.
                'numberValue': accuracy, # The value of the metric. Must be a numeric value.
                'format': "PERCENTAGE"   # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                }]
              }

              return metrics

          df = pd.read_csv(test_results)
          metrics = classification_metrics_helper(df)
          with open(mlpipeline_metrics, 'w') as f:
              json.dump(metrics, f)

      import argparse
      _parser = argparse.ArgumentParser(prog='Evaluate', description='The function evaluates the model performance.')
      _parser.add_argument("--test-results", dest="test_results", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = evaluate(**_parsed_args)
    args:
    - --test-results
    - {inputPath: test_results}
    - --mlpipeline-metrics
    - {outputPath: mlpipeline_metrics}
